{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch_dct import dct_2d\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from data.ImageDataset import ImageDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCTResNet(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(DCTResNet, self).__init__()\n",
    "        # Load a pretrained ResNet model\n",
    "        self.resnet = models.resnet101(pretrained=True)\n",
    "\n",
    "        # Modify the first convolutional layer to accept DCT input if needed\n",
    "        self.resnet.conv1 = nn.Conv2d(\n",
    "            3, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )  # Ensure it matches DCT input (3 channels)\n",
    "\n",
    "        # Modify the output layer to match the number of classes\n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 128),  # Add an intermediate FC layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes),  # Output layer\n",
    "            nn.Sigmoid()  # For binary classification\n",
    "        )\n",
    "\n",
    "    def apply_dct_batch(self, x):\n",
    "        \"\"\"\n",
    "        Applies DCT to a batch of images.\n",
    "        x: Tensor of shape (B, C, H, W)\n",
    "        \"\"\"\n",
    "        assert len(x.shape) == 4, \"Expected input tensor of shape (B, C, H, W)\"\n",
    "        # Apply DCT to each channel of each image in the batch\n",
    "        dct_images = torch.stack([dct_2d(x[:, c, :, :]) for c in range(x.shape[1])], dim=1)\n",
    "        return dct_images\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply DCT transformation\n",
    "        x = self.apply_dct_batch(x)\n",
    "        # Pass the DCT-transformed images through ResNet\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/CS230Project/venv/lib64/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ec2-user/CS230Project/venv/lib64/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "model = DCTResNet(num_classes=1)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),           \n",
    "    transforms.ToTensor(),                    \n",
    "    transforms.Normalize(                     \n",
    "        mean=[0.485, 0.456, 0.406], \n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(\n",
    "    annotations_path=\"/home/ec2-user/CS230Project/data/annotations/train.json\",\n",
    "    images_dir=\"/home/ec2-user/CS230Project/data/train\",\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "val_dataset = ImageDataset(\n",
    "    annotations_path=\"/home/ec2-user/CS230Project/data/annotations/val.json\",\n",
    "    images_dir=\"/home/ec2-user/CS230Project/data/val\",\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, num_workers=7,shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, num_workers=7, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/10:  11%|â–ˆ         | 74/690 [00:26<03:14,  3.17it/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device).float()  \n",
    "\n",
    "\n",
    "        outputs = model(images)\n",
    "        outputs = outputs.view(-1)  \n",
    "        labels = labels.view(-1)  \n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        predicted = (outputs > 0.5).float()  \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = 100. * correct / total\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
    "            images, labels = images.to(device), labels.to(device).float()\n",
    "\n",
    "            outputs = model(images)\n",
    "            outputs = outputs.view(-1)  \n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_accuracy = 100. * correct / total\n",
    "    print(f\"Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    checkpoint_path = f\"/home/ec2-user/CS230Project/code/models/saved-weights/DCTcnn/dct_cnn_{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    print(f\"Model saved to {checkpoint_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
